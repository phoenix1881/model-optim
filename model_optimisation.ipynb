{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Model Requirements and think which of these will do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from litgpt.lora import GPT\n",
    "from litgpt.tokenizer import Tokenizer\n",
    "from litgpt.prompts import PromptStyle\n",
    "import lightning as L\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# === Config ===\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "prompt_text = \"What food do llamas eat?\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Style the prompt\n",
    "prompt_style = PromptStyle.from_name(\"alpaca\")\n",
    "prompt = prompt_style.apply(prompt_text)\n",
    "\n",
    "# === Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "input_ids = inputs[\"input_ids\"]  # shape: [1, seq_len]\n",
    "\n",
    "# === Load model\n",
    "print(\"ðŸ”„ Loading model...\")\n",
    "t0 = time.perf_counter()\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "model = GPT.from_name(name=model_name)\n",
    "print(f\"âœ… Loaded in {time.perf_counter() - t0:.2f} seconds\")\n",
    "\n",
    "# === Finalize model\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.max_seq_length = input_ids.shape[-1]\n",
    "model.set_kv_cache(batch_size=16)\n",
    "model.cos, model.sin = model.rope_cache(device=device)\n",
    "L.seed_everything(42, verbose=False)\n",
    "\n",
    "# === Single-sample Benchmark\n",
    "print(\"\\n################ Single Inference Benchmark ################\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = model(input_ids)\n",
    "\n",
    "# Measure latency\n",
    "num_trials = 100\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start = time.time()\n",
    "    _ = model(input_ids)\n",
    "    latencies.append(time.time() - start)\n",
    "latencies = np.array(latencies)\n",
    "\n",
    "# Model size (float32 equivalent)\n",
    "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / 1e6\n",
    "\n",
    "print(\"\\nðŸ§ª Lit-GPT TinyLLaMA â€” Single Inference\")\n",
    "print(f\"Model Size: {model_size:.2f} MB (approx)\")\n",
    "print(f\"Median Latency: {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "print(f\"95th Percentile: {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "print(f\"99th Percentile: {np.percentile(latencies, 99)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {num_trials / np.sum(latencies):.2f} req/sec\")\n",
    "\n",
    "# === Batch Throughput Benchmark\n",
    "print(\"\\n################ Batch Inference Benchmark ################\")\n",
    "\n",
    "batch_size = 16\n",
    "batch_input = input_ids.repeat(batch_size, 1)  # shape: [batch, seq_len]\n",
    "batch_latencies = []\n",
    "num_batches = 50\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = model(batch_input)\n",
    "\n",
    "# Measure batch latencies\n",
    "for _ in range(num_batches):\n",
    "    start = time.time()\n",
    "    _ = model(batch_input)\n",
    "    batch_latencies.append(time.time() - start)\n",
    "\n",
    "total_tokens = batch_size * batch_input.shape[1] * num_batches\n",
    "batch_fps = total_tokens / np.sum(batch_latencies)\n",
    "\n",
    "print(\"\\nðŸ“¦ Lit-GPT TinyLLaMA â€” Batch Inference\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} tokens/sec\")\n",
    "print(f\"Avg Batch Latency: {np.mean(batch_latencies)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###add\n",
    "\n",
    "model.compile()\n",
    "\n",
    "##this line to above code and run again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from litgpt.lora import GPT\n",
    "from litgpt.prompts import PromptStyle\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "checkpoint_path = f\"../../checkpoints/{model_name}/lit_model.pth\"\n",
    "quant_path = \"../../quantized_litgpt_tinyllama.pt\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# === Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = \"Symptoms: fever and cough\\nQuestion: What should I do?\\nAnswer:\"\n",
    "prompt_style = PromptStyle.from_name(\"alpaca\")\n",
    "prompt = prompt_style.apply(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# === Load Lit-GPT model\n",
    "# with torch.device(\"meta\"):\n",
    "model = GPT.from_name(name=model_name)\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model = model.to_empty(device=\"cpu\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# === Quantize using PyTorch dynamic quant\n",
    "print(\"âš¡ Quantizing model...\")\n",
    "quantized_core = torch.quantization.quantize_dynamic(\n",
    "    model,  # âœ… quantize transformer core only\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# === Save quantized transformer core\n",
    "torch.save(quantized_core.state_dict(), quant_path)\n",
    "print(f\"âœ… Saved quantized Lit-GPT TinyLLaMA core model to: {quant_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on Quantised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from litgpt.lora import GPT\n",
    "from litgpt.prompts import PromptStyle\n",
    "import lightning as L\n",
    "\n",
    "# === Config\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "checkpoint_path = f\"../../checkpoints/{model_name}/lit_model.pth\"\n",
    "quant_path = \"../../quantized_litgpt_tinyllama.pt\"\n",
    "device = \"cpu\"  # dynamic quant runs on CPU\n",
    "\n",
    "# === Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Prompt\n",
    "prompt_text = \"What food do llamas eat?\"\n",
    "prompt = PromptStyle.from_name(\"alpaca\").apply(prompt_text)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# === Load GPT wrapper\n",
    "with torch.device(\"meta\"):\n",
    "    model = GPT.from_name(name=model_name)\n",
    "\n",
    "# Restore model with original shape\n",
    "model = model.to_empty(device=\"cpu\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "# === Inject quantized transformer core\n",
    "print(\"ðŸ”„ Loading quantized core...\")\n",
    "quantized_core = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_core.load_state_dict(torch.load(quant_path))\n",
    "model.model = quantized_core  # swap in quantized core\n",
    "model.eval()\n",
    "model.max_seq_length = input_ids.shape[-1]\n",
    "model.set_kv_cache(batch_size=16)\n",
    "model.cos, model.sin = model.rope_cache(device=\"cpu\")\n",
    "L.seed_everything(42, verbose=False)\n",
    "\n",
    "# === Single-sample Benchmark\n",
    "print(\"\\n################ Single Inference Benchmark ################\")\n",
    "for _ in range(5):  # warm-up\n",
    "    _ = model(input_ids)\n",
    "\n",
    "num_trials = 100\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start = time.time()\n",
    "    _ = model(input_ids)\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "print(f\"Median Latency: {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "print(f\"95th Percentile: {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "print(f\"99th Percentile: {np.percentile(latencies, 99)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {num_trials / np.sum(latencies):.2f} req/sec\")\n",
    "\n",
    "# === Batch Inference Benchmark\n",
    "print(\"\\n################ Batch Inference Benchmark ################\")\n",
    "batch_size = 16\n",
    "num_batches = 50\n",
    "batch_input = input_ids.repeat(batch_size, 1)\n",
    "batch_latencies = []\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = model(batch_input)\n",
    "\n",
    "# Benchmark\n",
    "for _ in range(num_batches):\n",
    "    start = time.time()\n",
    "    _ = model(batch_input)\n",
    "    batch_latencies.append(time.time() - start)\n",
    "\n",
    "total_tokens = batch_size * batch_input.shape[1] * num_batches\n",
    "batch_fps = total_tokens / np.sum(batch_latencies)\n",
    "\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} tokens/sec\")\n",
    "print(f\"Average Batch Latency: {np.mean(batch_latencies)*1000:.2f} ms\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
