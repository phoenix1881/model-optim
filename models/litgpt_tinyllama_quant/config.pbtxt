name: "litgpt_tinyllama_quant"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ -1 ]  # sequence length
  }
]
output [
  {
    name: "logits"
    data_type: TYPE_FP32
    dims: [ -1, 32000 ]  # adjust to vocab size
  }
]